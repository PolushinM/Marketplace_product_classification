{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "import multiprocessing\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ChainedScheduler, LinearLR, ExponentialLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizerFast\n",
    "\n",
    "def seed_everything(seed=42, deterministic=False):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = deterministic\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Основные настройки: seed, модель, рабочий каталог, warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n",
      "Device:  cuda\n",
      "CPU cores:  8\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "WORKDIR = '//home/ubuntu/gitrepo/KazanExpress/2/'\n",
    "IMAGES_FOLDER = os.path.join(WORKDIR, 'row_data/images/train/')\n",
    "IMAGES_FOLDER_TEST = os.path.join(WORKDIR, 'row_data/images/test/')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n",
    "seed_everything(SEED)\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device: \", device)\n",
    "print('CPU cores: ', multiprocessing.cpu_count())\n",
    "\n",
    "# =========================================================================================\n",
    "# Configurations\n",
    "# =========================================================================================\n",
    "class CFG:\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    model = \"openai/clip-vit-large-patch14\" \n",
    "    tokenizer = CLIPTokenizerFast.from_pretrained(model)\n",
    "    processor = CLIPProcessor.from_pretrained(model)\n",
    "    state_dict = None \n",
    "    max_length = 77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преобразование входных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from parquet\n",
    "data_full = pd.read_parquet(os.path.join(WORKDIR, 'row_data/train.parquet'))\n",
    "# Drop unnecessary columns\n",
    "data_full.drop(columns=['shop_id', 'rating'], inplace=True)\n",
    "data_full = data_full.drop(columns=['text_fields', 'shop_title', 'sale']).reset_index(drop=True)\n",
    "# Duplicate too rare values\n",
    "dup_ids = set(data_full.category_id.value_counts()[data_full.category_id.value_counts() < 2].index)\n",
    "data_full = data_full.append(data_full[data_full['category_id'].isin(dup_ids)])\n",
    "# Trait/test split\n",
    "data, data_valid = train_test_split(data_full, test_size=0.025, random_state=SEED, \n",
    "                                    shuffle=True, stratify=data_full.category_id)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data_valid.reset_index(drop=True, inplace=True)\n",
    "# Fix class numbers \n",
    "cls2id = data_full.category_id.unique()\n",
    "id2cls = {k : v for v, k in enumerate(cls2id)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Классы датасета и модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Dataset\n",
    "# =========================================================================================\n",
    "class doc_dataset(Dataset):\n",
    "    def __init__(self, targets: list, id2cls: dict, images_folder: str, product_ids:list, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.targets = targets\n",
    "        self.id2cls = id2cls\n",
    "        self.images_folder = images_folder\n",
    "        self.product_ids = product_ids\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image_inputs = self.cfg.processor(\n",
    "                text=None,\n",
    "                images=Image.open(os.path.join(self.images_folder, str(self.product_ids[item]) + '.jpg')),\n",
    "                return_tensors='pt'\n",
    "            )['pixel_values'][0]\n",
    "        return image_inputs, self.id2cls[self.targets[item]]\n",
    "\n",
    "# =========================================================================================\n",
    "# Image classification model\n",
    "# =========================================================================================\n",
    "class CLIP_CLF(nn.Module):\n",
    "    def __init__(self, cfg, n_classes, cut_emb=None, emb_act=None, bottleneck_size=None, bottleneck_act=None):\n",
    "        super().__init__()\n",
    "        # Configurations, CLIP model loading\n",
    "        self.cfg = cfg\n",
    "        self.config = AutoConfig.from_pretrained(cfg.model)\n",
    "        self.model = CLIPModel.from_pretrained(cfg.model)\n",
    "        # Truncate CLIP embeddings for dimension reduction\n",
    "        if cut_emb is not None:\n",
    "            self.in_size = cut_emb\n",
    "            self.cut_emb = True\n",
    "        else:\n",
    "            self.in_size = self.model.config.hidden_size\n",
    "            self.cut_emb = False\n",
    "        # Classifier layer\n",
    "        self.clf = nn.Linear(self.in_size, n_classes)\n",
    "\n",
    "    def forward(self, image_inputs):\n",
    "        # Get CLIP embeddings\n",
    "        emb = self.model.get_image_features(image_inputs)\n",
    "        # Truncate CLIP embeddings for dimension reduction\n",
    "        if self.cut_emb:\n",
    "            emb = emb[:, :self.in_size]\n",
    "        # Classifier\n",
    "        cls = self.clf(emb)\n",
    "        return cls\n",
    "    \n",
    "    def get_emb(self, image_inputs):\n",
    "        emb = self.model.get_image_features(image_inputs)\n",
    "        if self.cut_emb:\n",
    "            emb = emb[:, :self.in_size]\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция collate_fn и даталоадер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    doc_dataset(data.category_id.tolist(), id2cls, IMAGES_FOLDER, product_ids=data.product_id.tolist(), cfg=CFG), \n",
    "    batch_size = 24, \n",
    "    shuffle = True, \n",
    "    num_workers = CFG.num_workers, \n",
    "    pin_memory = True, \n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    doc_dataset(data_valid.category_id.tolist(), \n",
    "                id2cls, IMAGES_FOLDER, product_ids=data_valid.product_id.tolist(), cfg=CFG), \n",
    "    batch_size = 24, \n",
    "    shuffle = False, \n",
    "    num_workers = CFG.num_workers, \n",
    "    pin_memory = True, \n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, optimizer=None,\n",
    "          epochs=2, lr=0.0001, checkpoint_period=None, \n",
    "          warmup_epochs=2, gamma=0.925, verbose=True):\n",
    "    \n",
    "    if optimizer is None:\n",
    "        opt = Adam(model.parameters(), lr=lr * gamma ** -warmup_epochs)\n",
    "    else:\n",
    "        opt = optimizer\n",
    "    \n",
    "    model.to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()   \n",
    "    opt.zero_grad() \n",
    "    torch.cuda.empty_cache()\n",
    "    scheduler = ChainedScheduler([LinearLR(opt, start_factor=0.1, total_iters=warmup_epochs),\n",
    "                                  ExponentialLR(opt, gamma=gamma)])\n",
    "    if verbose:\n",
    "            print(f'Lr: {scheduler.get_last_lr()[0]:.9f}')\n",
    "    if checkpoint_period is None:\n",
    "        checkpoint_period = len(train_loader)\n",
    "    \n",
    "    max_f1 = 0.\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs+1, 1)):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        loss_avg = 0.\n",
    "        if verbose:\n",
    "            print(f'Epoch={epoch}')\n",
    "        for step, batch in tqdm(enumerate(train_loader), total=len(train_loader), disable = not verbose):\n",
    "            image_input = batch[0].to(device)\n",
    "            target = batch[1].to(device)\n",
    "            output = model(image_input)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            loss_avg += loss.item() / checkpoint_period\n",
    "            if step % checkpoint_period == checkpoint_period - 1:\n",
    "                if verbose:\n",
    "                    print(f'Step={step+1}, Train loss={loss_avg:.6f}')\n",
    "                loss_avg = 0.\n",
    "                torch.save(model.state_dict(), os.path.join(WORKDIR, 'checkpoints/checkpoint.pt'))\n",
    "                model.eval()\n",
    "                grun_truth = []\n",
    "                predicted = []                \n",
    "                with torch.no_grad():\n",
    "                    total = min(len(test_loader), (checkpoint_period // 3))\n",
    "                    for step, batch in enumerate(test_loader):\n",
    "                        image_input = batch[0].to(device)\n",
    "                        target = batch[1].to(device)\n",
    "                        output = model(image_input)\n",
    "                        loss = loss_fn(output, target)\n",
    "                        loss_avg += loss.item() / total\n",
    "                        grun_truth.append(target.cpu())\n",
    "                        predicted.append(output.argmax(dim=1).cpu())\n",
    "                        if step >= checkpoint_period // 3 - 1:\n",
    "                            break\n",
    "                weighted_f1 = f1_score(np.concatenate(grun_truth), np.concatenate(predicted), average='weighted')\n",
    "                \n",
    "                if weighted_f1 > max_f1:\n",
    "                    max_f1 = weighted_f1\n",
    "                    best_epoch = epoch\n",
    "                    torch.save(model.state_dict(), os.path.join(WORKDIR, 'checkpoints/best.pt'))\n",
    "                if verbose:\n",
    "                    print(f\"F1={weighted_f1:.5f}\")\n",
    "                    print(f'Eval loss={loss_avg:.5f}\\n')\n",
    "                loss_avg = 0.\n",
    "                model.train()\n",
    "                scheduler.step()\n",
    "                print(f'Lr: {scheduler.get_last_lr()[0]:.9f}')\n",
    "    return max_f1, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализация модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CLIP_CLF(CFG, len(cls2id), cut_emb=40).to(device)\n",
    "if CFG.state_dict is not None:\n",
    "    model.load_state_dict(CFG.state_dict)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lr: 0.000001331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2123a1855b4b518b18bb3c42966814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84eb4cf9a944baeb25ce5c0c17dff54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=600, Train loss=5.987983\n",
      "F1=0.20190\n",
      "Eval loss=5.47663\n",
      "\n",
      "Lr: 0.000002920\n",
      "Step=1200, Train loss=4.966136\n",
      "F1=0.32365\n",
      "Eval loss=4.54845\n",
      "\n",
      "Lr: 0.000004380\n",
      "Step=1800, Train loss=4.223240\n",
      "F1=0.41503\n",
      "Eval loss=3.88285\n",
      "\n",
      "Lr: 0.000005719\n",
      "Step=2400, Train loss=3.709109\n",
      "F1=0.47864\n",
      "Eval loss=3.40966\n",
      "\n",
      "Lr: 0.000006943\n",
      "Step=3000, Train loss=3.297177\n",
      "F1=0.51518\n",
      "Eval loss=3.03627\n",
      "\n",
      "Lr: 0.000008061\n",
      "Step=3600, Train loss=3.010670\n",
      "F1=0.54084\n",
      "Eval loss=2.80314\n",
      "\n",
      "Lr: 0.000009077\n",
      "Epoch=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e791e9b9f88c4e34aa60bbcd16b099d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=600, Train loss=2.421363\n",
      "F1=0.57048\n",
      "Eval loss=2.57067\n",
      "\n",
      "Lr: 0.000010000\n",
      "Step=1200, Train loss=2.301376\n",
      "F1=0.59912\n",
      "Eval loss=2.35871\n",
      "\n",
      "Lr: 0.000009600\n",
      "Step=1800, Train loss=2.179178\n",
      "F1=0.61606\n",
      "Eval loss=2.28136\n",
      "\n",
      "Lr: 0.000009216\n",
      "Step=2400, Train loss=2.097841\n",
      "F1=0.62628\n",
      "Eval loss=2.13938\n",
      "\n",
      "Lr: 0.000008847\n",
      "Step=3000, Train loss=1.944514\n",
      "F1=0.64871\n",
      "Eval loss=2.01130\n",
      "\n",
      "Lr: 0.000008493\n",
      "Step=3600, Train loss=1.876980\n",
      "F1=0.65448\n",
      "Eval loss=1.93320\n",
      "\n",
      "Lr: 0.000008154\n",
      "Epoch=3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe3ad966cdf442984fa5363f28bd711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=600, Train loss=1.166594\n",
      "F1=0.68168\n",
      "Eval loss=1.79811\n",
      "\n",
      "Lr: 0.000007828\n",
      "Step=1200, Train loss=1.134427\n",
      "F1=0.68043\n",
      "Eval loss=1.83395\n",
      "\n",
      "Lr: 0.000007514\n",
      "Step=1800, Train loss=1.131554\n",
      "F1=0.68128\n",
      "Eval loss=1.77260\n",
      "\n",
      "Lr: 0.000007214\n",
      "Step=2400, Train loss=1.085534\n",
      "F1=0.68090\n",
      "Eval loss=1.76795\n",
      "\n",
      "Lr: 0.000006925\n",
      "Step=3000, Train loss=1.062244\n",
      "F1=0.69995\n",
      "Eval loss=1.69791\n",
      "\n",
      "Lr: 0.000006648\n",
      "Step=3600, Train loss=1.045661\n",
      "F1=0.69690\n",
      "Eval loss=1.72919\n",
      "\n",
      "Lr: 0.000006382\n",
      "Epoch=4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46da80f74a4d02a61ac0d5ca505161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=600, Train loss=0.522503\n",
      "F1=0.72322\n",
      "Eval loss=1.58967\n",
      "\n",
      "Lr: 0.000006127\n",
      "Step=1200, Train loss=0.490513\n",
      "F1=0.72020\n",
      "Eval loss=1.62255\n",
      "\n",
      "Lr: 0.000005882\n",
      "Step=1800, Train loss=0.511207\n",
      "F1=0.71559\n",
      "Eval loss=1.67291\n",
      "\n",
      "Lr: 0.000005647\n",
      "Step=2400, Train loss=0.509138\n",
      "F1=0.71125\n",
      "Eval loss=1.62463\n",
      "\n",
      "Lr: 0.000005421\n",
      "Step=3000, Train loss=0.490567\n",
      "F1=0.70900\n",
      "Eval loss=1.63183\n",
      "\n",
      "Lr: 0.000005204\n",
      "Step=3600, Train loss=0.486236\n",
      "F1=0.71659\n",
      "Eval loss=1.64753\n",
      "\n",
      "Lr: 0.000004996\n",
      "Epoch=5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2873d7b48ff4cbbac9674515908b686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=600, Train loss=0.192422\n",
      "F1=0.72208\n",
      "Eval loss=1.61230\n",
      "\n",
      "Lr: 0.000004796\n",
      "Step=1200, Train loss=0.175703\n",
      "F1=0.72422\n",
      "Eval loss=1.66686\n",
      "\n",
      "Lr: 0.000004604\n",
      "Step=1800, Train loss=0.200432\n",
      "F1=0.72104\n",
      "Eval loss=1.70886\n",
      "\n",
      "Lr: 0.000004420\n",
      "Step=2400, Train loss=0.187584\n",
      "F1=0.71552\n",
      "Eval loss=1.68251\n",
      "\n",
      "Lr: 0.000004243\n",
      "Step=3000, Train loss=0.205775\n",
      "F1=0.72205\n",
      "Eval loss=1.71206\n",
      "\n",
      "Lr: 0.000004073\n",
      "Step=3600, Train loss=0.195540\n",
      "F1=0.72499\n",
      "Eval loss=1.71598\n",
      "\n",
      "Lr: 0.000003911\n",
      "Epoch=6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7fdd63c98f4840b7f6c66b0b4a5a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=600, Train loss=0.082378\n",
      "F1=0.72363\n",
      "Eval loss=1.69552\n",
      "\n",
      "Lr: 0.000003754\n",
      "Step=1200, Train loss=0.073777\n",
      "F1=0.72386\n",
      "Eval loss=1.71780\n",
      "\n",
      "Lr: 0.000003604\n",
      "Step=1800, Train loss=0.079297\n",
      "F1=0.72107\n",
      "Eval loss=1.72934\n",
      "\n",
      "Lr: 0.000003460\n",
      "Step=2400, Train loss=0.077488\n",
      "F1=0.71748\n",
      "Eval loss=1.73873\n",
      "\n",
      "Lr: 0.000003321\n",
      "Step=3000, Train loss=0.092139\n",
      "F1=0.71486\n",
      "Eval loss=1.76727\n",
      "\n",
      "Lr: 0.000003189\n",
      "Step=3600, Train loss=0.078627\n",
      "F1=0.72239\n",
      "Eval loss=1.73462\n",
      "\n",
      "Lr: 0.000003061\n",
      "Epoch=7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8231d3cadc480a9c19471fc48d4160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3702 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7231/3297760566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m train(model, train_loader, valid_loader, checkpoint_period=600,\n\u001b[0m\u001b[1;32m      5\u001b[0m       optimizer=None, warmup_epochs=7, epochs=20, lr=0.00001, gamma=0.96)\n",
      "\u001b[0;32m/tmp/ipykernel_7231/4044482694.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, optimizer, epochs, lr, checkpoint_period, warmup_epochs, gamma, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, valid_loader, checkpoint_period=600,\n",
    "      optimizer=None, warmup_epochs=7, epochs=20, lr=0.00001, gamma=0.96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем лучший чекпоинт:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(WORKDIR, 'checkpoints/best_clip.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции для генерации и проверки эмбеддингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_embeddings_eval(model) -> np.array:\n",
    "    full_loader = DataLoader(\n",
    "    doc_dataset(data_full.category_id.tolist(), \n",
    "                id2cls, IMAGES_FOLDER, product_ids=data_full.product_id.tolist(), cfg=CFG), \n",
    "    batch_size = 24, \n",
    "    shuffle = False, \n",
    "    num_workers = CFG.num_workers, \n",
    "    pin_memory = True, \n",
    "    drop_last = False\n",
    ")\n",
    "    outputs = []\n",
    "    grun_truth = []\n",
    "    predicted = []\n",
    "    model.eval()\n",
    "    total = len(full_loader)\n",
    "    for step, batch in tqdm(enumerate(full_loader), total=total):\n",
    "        with torch.no_grad():\n",
    "            image_input = batch[0].to(device)\n",
    "            target = batch[1].to(device)\n",
    "            output = model.get_emb(image_input)\n",
    "            outputs.append(model.clf(output).cpu().numpy())\n",
    "            grun_truth.append(target.cpu())\n",
    "            predicted.append(output.argmax(dim=1).cpu())\n",
    "    weighted_f1 = f1_score(np.concatenate(grun_truth), np.concatenate(predicted), average='weighted')\n",
    "    print(f\"F1={weighted_f1:.5f}\")\n",
    "    return np.concatenate(outputs)\n",
    "\n",
    "def get_embeddings(model, test_loader) -> np.array:\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    total = len(test_loader)\n",
    "    for step, batch in tqdm(enumerate(test_loader), total=total):\n",
    "        with torch.no_grad():\n",
    "            image_input = batch[0].to(device)\n",
    "            output = model.get_emb(image_input)\n",
    "            outputs.append(output.cpu().numpy())\n",
    "    return np.concatenate(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем размерности и воспроизводимость генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff52cfcc3b641c192448fdec5f91768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3797 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_new = get_full_embeddings_eval(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91124, 40)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91124, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_old = np.load('embeddings_clip.np.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embeddings_old - embeddings_new).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Читаем данные из тестового датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from parquet\n",
    "data_test = pd.read_parquet(os.path.join(WORKDIR, 'row_data/test.parquet'))\n",
    "# Drop unnecessary columns\n",
    "data_test.drop(columns=['shop_id', 'rating'], inplace=True)\n",
    "data_test = data_test.drop(columns=['text_fields', 'shop_title', 'sale']).reset_index(drop=True)\n",
    "data_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    doc_dataset([2789] * len(data_test.product_id.tolist()), id2cls, \n",
    "                IMAGES_FOLDER_TEST, product_ids=data_test.product_id.tolist(), cfg=CFG), \n",
    "    batch_size = 24, \n",
    "    shuffle = False, \n",
    "    num_workers = CFG.num_workers, \n",
    "    pin_memory = True, \n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируем эмбеддинги для тестового сета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66b9ad93a8a4929926b69f75afc91f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/703 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_test = get_embeddings(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16860, 40)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16860, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings_clip_test.np', embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "73baa7047eb6901c2be83950c21fe663ea57cccf327cab8d2ef6784beacf294e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
