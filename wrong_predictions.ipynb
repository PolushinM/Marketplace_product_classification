{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T12:48:40.281315Z","iopub.status.busy":"2023-03-19T12:48:40.280943Z","iopub.status.idle":"2023-03-19T12:48:44.683544Z","shell.execute_reply":"2023-03-19T12:48:44.682333Z","shell.execute_reply.started":"2023-03-19T12:48:40.281282Z"},"trusted":true},"outputs":[],"source":["import os\n","import random\n","import gc\n","import multiprocessing\n","import warnings\n","from tqdm.auto import tqdm\n","\n","import numpy as np \n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data import default_collate\n","\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import DataCollatorWithPadding\n","\n","from utils import get_title, preprocess_text_field, MeanPooling, Attention\n","\n","def seed_everything(seed=42, deterministic=False):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = deterministic\n","    torch.backends.cudnn.benchmark = False"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Основные настройки: seed, модель, рабочий каталог, warnings."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T12:48:44.686340Z","iopub.status.busy":"2023-03-19T12:48:44.686049Z","iopub.status.idle":"2023-03-19T12:48:48.181985Z","shell.execute_reply":"2023-03-19T12:48:48.180946Z","shell.execute_reply.started":"2023-03-19T12:48:44.686310Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["env: TOKENIZERS_PARALLELISM=false\n","Device:  cuda\n","CPU cores:  6\n"]}],"source":["SEED = 42\n","WORKDIR = '/home/maksim/KazanExpress/2/'\n","IMAGES_FOLDER = os.path.join(WORKDIR, 'row_data/images/train/')\n","warnings.filterwarnings(\"ignore\")\n","os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n","seed_everything(SEED)\n","\n","%env TOKENIZERS_PARALLELISM=false\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device: \", device)\n","print('CPU cores: ', multiprocessing.cpu_count())\n","\n","# =========================================================================================\n","# Configurations\n","# =========================================================================================\n","class CFG:\n","    num_workers = multiprocessing.cpu_count()\n","    clip_embeddings = np.load(os.path.join(WORKDIR, 'embeddings.np.npy'))\n","    clip_cut_emb = 32\n","    bert_model = 'cointegrated/rubert-tiny2' \n","    bert_tokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny2')\n","    bert_cut_emb = None\n","    state_dict = torch.load(os.path.join(WORKDIR, 'checkpoint_clip_bert_kaggle.pt'))\n","    max_length = 256\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Преобразование входных данных"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T13:25:13.053624Z","iopub.status.busy":"2023-03-19T13:25:13.052699Z","iopub.status.idle":"2023-03-19T13:25:20.015958Z","shell.execute_reply":"2023-03-19T13:25:20.014906Z","shell.execute_reply.started":"2023-03-19T13:25:13.053574Z"},"trusted":true},"outputs":[],"source":["# Read from parquet\n","data_full = pd.read_parquet(os.path.join(WORKDIR, 'row_data/train.parquet'))\n","# Drop unnecessary columns\n","data_full.drop(columns=['shop_id', 'rating'], inplace=True)\n","# Convert text fields\n","data_full['title'] = data_full.text_fields.apply(get_title)\n","data_full.text_fields = data_full.text_fields.apply(preprocess_text_field)\n","# Convert \"Sale\"\n","data_full['sale'] = data_full['sale'].apply(lambda x: \"Распродажа!\" if x else \"\")  \n","data_full.fillna(value='', inplace=True)\n","# Concatenate to one string\n","data_full = data_full.assign(Document=[str(y) + ': ' + str(x) + '. ' + str(z) + '. ' + str(s) + '. ' \\\n","                                       for x, y, z, s in zip(data_full['title'], data_full['shop_title'],\n","                                                           data_full['text_fields'], data_full['sale'])])\n","\n","data_full = data_full.drop(columns=['text_fields', 'shop_title', 'sale', 'title']).reset_index(drop=True)\n","# Drop too rare values\n","drop_ids = set(data_full.category_id.value_counts()[data_full.category_id.value_counts() < 2].index)\n","data_full = data_full[~data_full['category_id'].isin(drop_ids)]\n","# Trait/test split\n","if CFG.clip_embeddings is not None:\n","    data, data_valid, clip_embeddings, clip_embeddings_valid = train_test_split(data_full, CFG.clip_embeddings, \n","                                                                    test_size=0.2, random_state=SEED, \n","                                                                    shuffle=True, stratify=data_full.category_id)\n","else:\n","    data, data_valid_stack = train_test_split(data_full, test_size=0.2, random_state=SEED, \n","                                        shuffle=True, stratify=data_full.category_id)\n","    \n","data.reset_index(drop=True, inplace=True)\n","data_valid.reset_index(drop=True, inplace=True)\n","# Fix class umbers \n","cls2id = data_full.category_id.unique()\n","id2cls = {k : v for v, k in enumerate(cls2id)}\n","# Make categories dictionary\n","id2category = {k:v[15:] for k, v in zip(data_full.category_id.tolist(), data_full.category_name.tolist())}\n","# del data_full"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Классы датасета и модели. \n","Модель и датасет позволяют загружать как модель CLIP (собственную или с huggingface), так и готовые (ранее сгенерированные и сохранунные в numpy array) эмбеддинги CLIP."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T12:48:54.685665Z","iopub.status.busy":"2023-03-19T12:48:54.685284Z","iopub.status.idle":"2023-03-19T12:48:54.709290Z","shell.execute_reply":"2023-03-19T12:48:54.707979Z","shell.execute_reply.started":"2023-03-19T12:48:54.685625Z"},"trusted":true},"outputs":[],"source":["# =========================================================================================\n","# Dataset\n","# =========================================================================================\n","class stacked_dataset(Dataset):\n","    def __init__(self, cfg, documents:list, targets: list, \n","                 id2cls: dict, images_folder: str, \n","                 product_ids:list, clip_embeddings=None):\n","        \n","        if clip_embeddings is not None:\n","            self.use_precalculated_clip_embs = True\n","            self.clip_embeddings = clip_embeddings\n","        else:\n","            self.use_precalculated_clip_embs = False\n","        \n","        self.cfg = cfg\n","        self.data = documents\n","        self.targets = targets\n","        self.id2cls = id2cls\n","        self.images_folder = images_folder\n","        self.product_ids = product_ids\n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, item):\n","        if self.use_precalculated_clip_embs:\n","            image_inputs = self.clip_embeddings[item][None, :]\n","        else:\n","            image = Image.open(os.path.join(self.images_folder, str(self.product_ids[item]) + '.jpg'))\n","            image_inputs = self.cfg.clip_processor(\n","                    text=None,\n","                    images=image,\n","                    return_tensors='pt'\n","                )['pixel_values']\n","        text_inputs=self.cfg.bert_tokenizer(\n","                self.data[item], \n","                return_tensors=None, \n","                add_special_tokens=True, \n","                max_length=self.cfg.max_length,\n","                truncation=True\n","            )\n","        return text_inputs, image_inputs, self.id2cls[self.targets[item]]\n","\n","# =========================================================================================\n","# Classifier model\n","# =========================================================================================   \n","class STACKED_CLF(nn.Module):\n","    def __init__(self, cfg, n_classes):\n","        super().__init__()\n","        # Configurations, CLIP and BERT models loading\n","        self.cfg = cfg\n","        if cfg.clip_embeddings is None:\n","            self.clip_config = AutoConfig.from_pretrained(cfg.model)\n","            self.clip_model = CLIPModel.from_pretrained(cfg.model)\n","        else: \n","            self.use_precalculated_clip_embs = True\n","        self.bert_config = AutoConfig.from_pretrained(cfg.bert_model)\n","        self.bert_model = AutoModel.from_pretrained(cfg.bert_model, config = self.bert_config)\n","        self.bert_pool = MeanPooling()\n","        # CLIP embeddings from model or precalculated\n","        if cfg.bert_cut_emb is None:\n","            self.hidden_dim = self.bert_model.config.hidden_size + cfg.clip_cut_emb\n","        else:\n","            self.hidden_dim = cfg.bert_cut_emb + cfg.clip_cut_emb\n","        # Attentions\n","        self.attention_clip = Attention(self.hidden_dim-cfg.clip_cut_emb, cfg.clip_cut_emb)\n","        self.attention_bert = Attention(cfg.clip_cut_emb, self.hidden_dim-cfg.clip_cut_emb)\n","        # Classifier\n","        self.bn = nn.BatchNorm1d(self.hidden_dim)\n","        self.clf = nn.Linear(self.hidden_dim, n_classes)\n","\n","    def forward(self, text_inputs, image_inputs):\n","        # Get BERT embeddings from text\n","        text_emb = self.bert_model(**text_inputs)\n","        text_emb = self.bert_pool(text_emb.last_hidden_state, text_inputs['attention_mask']) \n","        # Get CLIP embeddings from pictures\n","        if self.use_precalculated_clip_embs:\n","            img_emb = image_inputs \n","        else:\n","            img_emb = self.clip_model.get_image_features(image_inputs)\n","        # Cut embeddings\n","        text_emb = text_emb[:, :self.cfg.bert_cut_emb]\n","        img_emb = img_emb[:, :self.cfg.clip_cut_emb]\n","        # Apply attentions\n","        img_emb = self.attention_clip(text_emb, img_emb)\n","        text_emb = self.attention_bert(img_emb, text_emb)\n","        # Concatenate BERT and CLIP embeddings\n","        emb  = torch.cat([text_emb, img_emb], dim=1).float()\n","        # Classifier\n","        cls = self.clf(self.bn(emb))\n","        return cls"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Функция collate_fn и даталоадер:"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T12:48:54.711665Z","iopub.status.busy":"2023-03-19T12:48:54.710925Z","iopub.status.idle":"2023-03-19T12:48:54.731828Z","shell.execute_reply":"2023-03-19T12:48:54.730616Z","shell.execute_reply.started":"2023-03-19T12:48:54.711627Z"},"trusted":true},"outputs":[],"source":["transformers_collator = DataCollatorWithPadding(tokenizer = CFG.bert_tokenizer, padding = 'longest')\n","\n","def custom_collate(batch):\n","    texts_batch = []\n","    images_batch = []\n","    targets_batch = []\n","    for item in batch:\n","        texts_batch.append(item[0])\n","        images_batch.append(item[1][0])\n","        targets_batch.append(item[2])\n","    text_inputs = transformers_collator(texts_batch)\n","    return text_inputs, default_collate(images_batch), default_collate(targets_batch)\n","\n","valid_loader = DataLoader(\n","    stacked_dataset(CFG, documents=data_valid.Document.tolist(), targets=data_valid.category_id.tolist(), \n","                  id2cls=id2cls, images_folder=IMAGES_FOLDER, \n","                  product_ids=data_valid.product_id.tolist(), clip_embeddings=clip_embeddings_valid), \n","    batch_size = 256, \n","    shuffle = False, \n","    collate_fn = custom_collate,\n","    num_workers = CFG.num_workers, \n","    pin_memory = True, \n","    drop_last = False\n",")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Создание модели:"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T12:48:54.757877Z","iopub.status.busy":"2023-03-19T12:48:54.756889Z","iopub.status.idle":"2023-03-19T12:48:58.235133Z","shell.execute_reply":"2023-03-19T12:48:58.234079Z","shell.execute_reply.started":"2023-03-19T12:48:54.757842Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"data":{"text/plain":["17"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["model = STACKED_CLF(CFG, len(cls2id)).to(device)\n","if CFG.state_dict is not None:\n","    model.load_state_dict(CFG.state_dict)\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def get_categories(model, test_loader):\n","    grun_truth = []\n","    predicted = []\n","    model.to(device)\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in tqdm(test_loader, total = len(test_loader)):\n","            text_input = batch[0].to(device)\n","            image_input = batch[1].to(device)\n","            target = batch[2].to(device)\n","            output = model(text_input, image_input)\n","            grun_truth.append(target.cpu())\n","            predicted.append(output.argmax(dim=1).cpu())\n","    grun_truth = np.concatenate(grun_truth)\n","    predicted = np.concatenate(predicted)\n","    weighted_f1 = f1_score(grun_truth, predicted, average='weighted')\n","    print(f\"F1={weighted_f1:.5f}\")\n","    return predicted"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Создаем датафрейм для просмотра ошибочно предсказанных категорий. Тот же сплит, что и в начале."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["data_full = pd.read_parquet(os.path.join(WORKDIR, 'row_data/train.parquet'))\n","data_full = data_full[~data_full['category_id'].isin(drop_ids)]\n","_, data_watch, _, _ = train_test_split(data_full, data_full, test_size=0.2, random_state=SEED, \n","                                        shuffle=True, stratify=data_full.category_id)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Делаем предсказание категорий:"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34b7b1c43eb641c2835ecce807c9567a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/72 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 5.93 GiB total capacity; 2.10 GiB already allocated; 81.00 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predicted \u001b[39m=\u001b[39m get_categories(model, valid_loader)\n","Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mget_categories\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m image_input \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m target \u001b[39m=\u001b[39m batch[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m output \u001b[39m=\u001b[39m model(text_input, image_input)\n\u001b[1;32m     12\u001b[0m grun_truth\u001b[39m.\u001b[39mappend(target\u001b[39m.\u001b[39mcpu())\n\u001b[1;32m     13\u001b[0m predicted\u001b[39m.\u001b[39mappend(output\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu())\n","File \u001b[0;32m~/gitrepo/Jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[4], line 74\u001b[0m, in \u001b[0;36mSTACKED_CLF.forward\u001b[0;34m(self, text_inputs, image_inputs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, image_inputs):\n\u001b[1;32m     73\u001b[0m     \u001b[39m# Get BERT embeddings from text\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     text_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtext_inputs)\n\u001b[1;32m     75\u001b[0m     text_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert_pool(text_emb\u001b[39m.\u001b[39mlast_hidden_state, text_inputs[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]) \n\u001b[1;32m     76\u001b[0m     \u001b[39m# Get CLIP embeddings from pictures\u001b[39;00m\n","File \u001b[0;32m~/gitrepo/Jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/gitrepo/Jupyter/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1019\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1010\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1012\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1013\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1014\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1018\u001b[0m )\n\u001b[0;32m-> 1019\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1020\u001b[0m     embedding_output,\n\u001b[1;32m   1021\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1022\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1023\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1024\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1026\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1027\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1028\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1029\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1030\u001b[0m )\n\u001b[1;32m   1031\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1032\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/gitrepo/Jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/gitrepo/Jupyter/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:609\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    600\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    601\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    602\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[1;32m    608\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    610\u001b[0m         hidden_states,\n\u001b[1;32m    611\u001b[0m         attention_mask,\n\u001b[1;32m    612\u001b[0m         layer_head_mask,\n\u001b[1;32m    613\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    614\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    615\u001b[0m         past_key_value,\n\u001b[1;32m    616\u001b[0m         output_attentions,\n\u001b[1;32m    617\u001b[0m     )\n\u001b[1;32m    619\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    620\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n","File \u001b[0;32m~/gitrepo/Jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/gitrepo/Jupyter/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    493\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    496\u001b[0m         hidden_states,\n\u001b[1;32m    497\u001b[0m         attention_mask,\n\u001b[1;32m    498\u001b[0m         head_mask,\n\u001b[1;32m    499\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    500\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    504\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[0;32m~/gitrepo/Jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/gitrepo/Jupyter/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 425\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[1;32m    427\u001b[0m         attention_mask,\n\u001b[1;32m    428\u001b[0m         head_mask,\n\u001b[1;32m    429\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    431\u001b[0m         past_key_value,\n\u001b[1;32m    432\u001b[0m         output_attentions,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    435\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m~/gitrepo/Jupyter/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/gitrepo/Jupyter/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:363\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m     attention_probs \u001b[39m=\u001b[39m attention_probs \u001b[39m*\u001b[39m head_mask\n\u001b[0;32m--> 363\u001b[0m context_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(attention_probs, value_layer)\n\u001b[1;32m    365\u001b[0m context_layer \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m    366\u001b[0m new_context_layer_shape \u001b[39m=\u001b[39m context_layer\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size,)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 78.00 MiB (GPU 0; 5.93 GiB total capacity; 2.10 GiB already allocated; 81.00 MiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["predicted = get_categories(model, valid_loader)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Выбираем ошибочно предсказанные, приводим к читаемому виду:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T13:20:36.090858Z","iopub.status.busy":"2023-03-19T13:20:36.090102Z","iopub.status.idle":"2023-03-19T13:20:36.109182Z","shell.execute_reply":"2023-03-19T13:20:36.107751Z","shell.execute_reply.started":"2023-03-19T13:20:36.090817Z"},"trusted":true},"outputs":[],"source":["data_watch['predicted'] = [cls2id[x] for x in predicted] \n","data_watch_wrong = data_watch[data_watch.category_id != data_watch.predicted]\n","data_watch_wrong['predicted_category'] = data_watch_wrong.predicted.apply(lambda x: id2category[x])\n","data_watch_wrong.category_name = data_watch_wrong.category_name.apply(lambda x: x[15:])\n","data_watch_wrong.drop(columns=['product_id', 'sale', 'sale', 'shop_id', 'shop_title', 'rating', 'category_id', 'predicted'], inplace=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Посмотрим на ошибки:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T13:20:36.110595Z","iopub.status.busy":"2023-03-19T13:20:36.110330Z","iopub.status.idle":"2023-03-19T13:20:36.120164Z","shell.execute_reply":"2023-03-19T13:20:36.118968Z","shell.execute_reply.started":"2023-03-19T13:20:36.110569Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{\"title\": \"Картина по номерам на холсте с подрамником 30х40 см \\\"Китайский дракон\\\"\", \"description\": \"<iframe class=\\\"ql-video\\\" src=\\\"https://www.youtube.com/embed/MBbjACIcU8s?showinfo=0&amp;rel=0&am\n","TRUE: Хобби и творчество->Создание картин, фоторамок, открыток->Картины по номерам->Другое\n","PRED: Хобби и творчество->Создание картин, фоторамок, открыток->Картины по номерам->Животные и птицы \n","\n","{\"title\": \"Игровая клавиатура Thunder Wolf T20\", \"description\": \"<p>Игровая клавиатура Thunder Wolf T20 отлично подходит для работы и игр в темноте. <span style=\\\"color: rgb(51, 51, 51); background-co\n","TRUE: Электроника->Игровые приставки->Игровые контроллеры->Игровые клавиатуры\n","PRED: Электроника->Компьютерная техника->Аксессуары для компьютеров->Клавиатуры \n","\n","{\"title\": \"Интерьерная табличка, декоративное панно\", \"description\": \"<p>Яркая деревянная картина \\\"Выходя из дома не забудь\\\", \\\"Правила этого дома\\\", \\\"Никогда не теряй терпения\\\", \\\"Благословение д\n","TRUE: Товары для дома->Декор и интерьер->Картины, панно и постеры->Панно\n","PRED: Товары для дома->Декор и интерьер->Таблички, номера и крючки \n","\n","{\"title\": \"Картина по номерам на холсте с подрамником \\\"Двухцветный попугай\\\", 50x40 см\", \"description\": \"<iframe class=\\\"ql-video\\\" frameborder=\\\"0\\\" allowfullscreen=\\\"true\\\" src=\\\"https://www.youtub\n","TRUE: Хобби и творчество->Создание картин, фоторамок, открыток->Картины по номерам->Другое\n","PRED: Хобби и творчество->Создание картин, фоторамок, открыток->Картины по номерам->Животные и птицы \n","\n","{\"title\": \"Еловая ветка с красными ягодами.  Длина 22 см\", \"description\": \"<p>Красивая декоративная ветка с красными ягодами, припорошенными снегом.</p><p>Прекрасный вариант для дополнения новогодних \n","TRUE: Товары для дома->Товары для праздников->Новогодние товары->Ёлки искусственные\n","PRED: Товары для дома->Декор и интерьер->Искусственные цветы \n","\n","{\"title\": \"Высокие зеленые носки с клоуном\", \"description\": \"<p>Хлопковые, длинные носки с вышитыми рисунками.</p><p>Пятка и носок выполнены в голубом цвете</p>\", \"attributes\": [\"Материал: хлопок (70%\n","TRUE: Одежда->Мужская одежда->Носки->Носки и подследники\n","PRED: Одежда->Женская одежда->Колготки, носки, чулки->Носки и подследники \n","\n","{\"title\": \"Настенный стеллаж для хранения специй и банки для специй\", \"description\": \"<p class=\\\"ql-align-justify\\\"><strong>Настенный стеллаж позволит Вам организовать пространства на кухне, сохранить\n","TRUE: Товары для дома->Товары для кухни->Порядок на кухне->Полки кухонные\n","PRED: Товары для дома->Товары для кухни->Хранение продуктов->Емкости для специй и мельницы \n","\n","{\"title\": \"Наклейка знак \\\"Не мусорить\\\", 18х18 см\", \"description\": \"<p>Наклейка знак \\\"Не мусорить\\\", 18х18 см</p><p><br></p>\", \"attributes\": [], \"custom_characteristics\": {}, \"defined_characteristic\n","TRUE: Товары для дома->Декор и интерьер->Таблички, номера и крючки\n","PRED: Товары для дома->Хозяйственные товары->Мусорные ведра и баки \n","\n","{\"title\": \"Шлейф SATA 3.0, 0.45m, Red\", \"description\": \"<p><strong class=\\\"ql-size-large\\\">Характеристики:</strong></p><p><br></p><ul><li><em class=\\\"ql-size-large\\\">Цвет: </em><strong class=\\\"ql-size\n","TRUE: Электроника->Аксессуары для электроники->Кабели\n","PRED: Одежда->Спецодежда->Аксессуары для спецодежды \n","\n","{\"title\": \"Проводные детские наушники HELLO KITTY KT-3156\", \"description\": \"<p>Наушники KT-3156 – это полноценные детские наушники с изображениями известных мультяшных героев, которые не оставят равно\n","TRUE: Электроника->Наушники и аудиотехника->Наушники->Проводные наушники\n","PRED: Электроника->Наушники и аудиотехника->Наушники->Беспроводные наушники \n","\n","{\"title\": \"Подставка под губку на присосках \\\"Тиффани\\\",  бежевый\", \"description\": \"<p><strong>Стильная и удобная подставка для губки украсит Вашу кухню или в ванную комнату, при этом всегда будет под\n","TRUE: Товары для дома->Товары для кухни->Порядок на кухне->Принадлежности для мытья посуды->Щетки и губки\n","PRED: Товары для дома->Товары для кухни->Порядок на кухне->Подставки и держатели \n","\n","{\"title\": \"Контейнер для хранения пищевых, сыпучих продуктов\", \"description\": \"<p>Герметичный и безопасный контейнер для хранения продуктов позволяет навести порядок на кухне. Контейнеры для еды позво\n","TRUE: Товары для дома->Товары для кухни->Хранение продуктов->Банки и крышки\n","PRED: Товары для дома->Товары для кухни->Хранение продуктов->Контейнеры и ланч-боксы \n","\n","{\"title\": \"Топ бюстгалтер женский базовый, без косточек, для спорта и фитнеса\", \"description\": \"<p><strong>Универсальный женский топ бюстгалтер на каждый день!</strong></p><p>Подходит как для повседне\n","TRUE: Одежда->Женская одежда->Белье и купальники->Бюстгальтеры и аксессуары->Бюстгальтеры\n","PRED: Одежда->Женская одежда->Белье и купальники->Майки и топы бельевые \n","\n","{\"title\": \"Письмо Деду Морозу и Снегурочке - 3 шт\", \"description\": \"<h2>Описание</h2><p>Написание ежегодного Письма Деду Морозу и Снегурочке может стать приятной семейной традицией. Родители смогут уз\n","TRUE: Товары для дома->Товары для праздников->Открытки и конверты->Открытки\n","PRED: Товары для дома->Товары для праздников->Открытки и конверты->Конверты \n","\n","{\"title\": \"Основа для кольца\", \"description\": \"<ul><li>Материал: нержавеющая сталь</li><li>Цена за штуку</li><li>Диаметр 12 мм</li><li>Само кольцо регулируется под любой размер</li></ul>\", \"attributes\n","TRUE: Хобби и творчество->Рукоделие->Создание украшений\n","PRED: Хобби и творчество->Рукоделие->Материалы для рукоделия \n","\n","{\"title\": \"Женские домашние тапочки\", \"description\": \"<p>Не скользят</p><p>Мягкие и удобные</p><p>Живые фото:</p><p><br></p><p><img src=\\\"https://image-cdn.kazanexpress.ru/c814n2qe54e12u7keg20/origina\n","TRUE: Обувь->Обувь для девочек->Домашняя обувь\n","PRED: Обувь->Женская обувь->Домашняя обувь \n","\n","{\"title\": \"Картина по номерам 40*50 на подрамнике \\\"Свидание в Париже\\\" Q5352\", \"description\": \"<p>Картина по номерам 40*50 на подрамнике \\\"Свидание в Париже\\\" Q5352&nbsp;- открыв ее, можно сразу начи\n","TRUE: Хобби и творчество->Создание картин, фоторамок, открыток->Картины по номерам->Романтика и любовь\n","PRED: Хобби и творчество->Создание картин, фоторамок, открыток->Картины по номерам->Люди \n","\n","{\"title\": \"Фотофон 100*200 см\", \"description\": \"<p>Идеальное решение и для праздничного оформления интерьера или фотозоны, и для декорирования латексных и фольгированных шаров. Мерцающий занавес помож\n","TRUE: Товары для дома->Товары для праздников->Оформление праздника->Занавесы для фотозоны\n","PRED: Электроника->Фото- и видеотехника->Оборудование для фотографов->Фотофоны \n","\n","{\"title\": \"Носки детские\", \"description\": \"<p>Носки детские разного цвета с рисунком смайл.</p><p>Ткань: 80% хлопок, 15% лайкра. 5% полиамид.</p><p>Размер: от 1 года до 6 лет.</p>\", \"attributes\": [\"Тк\n","TRUE: Одежда->Детская одежда->Одежда для девочек->Носки и колготки->Носки и подследники\n","PRED: Одежда->Детская одежда->Одежда для мальчиков->Носки->Носки и подследники \n","\n","{\"title\": \"Пипетка мерная для растворов, пипетка для отливки мармелада 5 мл\", \"description\": \"<p>Пипетка сделана из высококачественных материалов, имеет дозатор для равномерной подачи жидкости. Благод\n","TRUE: Товары для дома->Товары для кухни->Формы для приготовления->Формы для леденцов и конфет\n","PRED: Товары для дома->Товары для кухни->Аксессуары для выпечки->Мешки, шприцы и насадки \n","\n"]}],"source":["for i, row in data_watch_wrong.head(20).iterrows():\n","    print(str(row[0])[:200])\n","    print('TRUE:', row[1])\n","    print('PRED:', row[2], '\\n')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Комментарий:\n","В тестовой выборке получилось 1988 (10% от исходного количества) товаров, на которых модель ошиблась, смотрим, что это за товары:\n","\n","Пример, когда модель предсказывает правильно, но категория все равно не совпадает с разметкой (таких примеров много):\n","\"{\"title\": \"Картина по номерам на холсте с подрамником \\\"Двухцветный попугай\\\", 50x40 см\"}\n","TRUE: Хобби и творчество->Создание картин, фоторамок, открыток->Картины по номерам->Другое\n","PRED: Хобби и творчество->Создание картин, фоторамок, открыток->Картины по номерам->Животные и птицы\"\n","\n","Пример, когда модель предсказывает неплохо, но немного не попадает в категорию (таких примеров большинство):\n","\"{\"title\": \"Настенный стеллаж для хранения специй и банки для специй\"}\n","TRUE: Товары для дома->Товары для кухни->Порядок на кухне->Полки кухонные\n","PRED: Товары для дома->Товары для кухни->Хранение продуктов->Емкости для специй и мельницы\"\n","\n","Пример грубой ошибки модели (подобных ошибок среди первых 20 неверно предсказанных я нашел всего 4, т.е. 2% от исходного количества):\n","\"{\"title\": \"Наклейка знак \\\"Не мусорить\\\", 18х18 см\"}\n","TRUE: Товары для дома->Декор и интерьер->Таблички, номера и крючки\n","PRED: Товары для дома->Хозяйственные товары->Мусорные ведра и баки\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"73baa7047eb6901c2be83950c21fe663ea57cccf327cab8d2ef6784beacf294e"}}},"nbformat":4,"nbformat_minor":4}
