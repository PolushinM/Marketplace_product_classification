### Описание задачи:
Научиться предсказывать категорию на основе описания, картинки и других параметрах товаров.
Дано: около 840 категорий, 91120 товаров в трейне и 16860 в тесте.

#### Краткий обзор существующих решений.
Задача похожа на задачу, которую я решал при отборе на стажировку в 2022 году: https://github.com/PolushinM/Hierarchical-Classifier.
Также у этой задачи много общего с задачей матчинга товаров, которая очень часто решается в маркетплейсах, много идей можно почерпнуть из этих статей: [Раз](https://habr.com/ru/company/aliexpress_russia/blog/686672/), [два](https://habr.com/ru/company/aliexpress_russia/blog/660757/).

#### В полученном для работы присутствуют следующие свойства объектов:
**product_id** - идентификатор товара, который необходимо классифицировать. \
**text_fields** - словарь, сериализованный в JSON со следующими полями: \
    **title** - заголовок \
    **description** - краткое описание товара\
    **attributes** - на сайте они находятся в карточке товара (Краткое описание товара) \
    **custom_characteristics, defined_characteristics, filters** - вложенные словари с ключами – названиями полей-признаков и значениями – возможными значениями этих признаков (например, Цвет: Белый / Черный) \
    **rating** - средний рейтинг товара. Эту информацию необходимо удалить - рейтинг товара неизвестен на момент добавления его в каталог (это будет лик). \
    **sale** - флаг, обозначающий находится ли товар в распродаже. Если товар на распродаже, я добавляю в его описание слово "Распродажа" .\
**shop_id** (seller_id) - id магазина (id-продавца). Эту колонку я удалил: наименование магазина уже есть, и оно даст больше информации. \
**shop_title** - название магазина \
**category_id** - категория товара (таргет) \
**category_name** - название категории товара с точки зрения дерева категорий KazanExpress. Теоретически, наименование категории тоже можно использовать для классификации, но это очень сильно усложняет пайплайн. Например, можно вычислить эмбеддинги категории, и искать ближайших соседей для товара среди этих категорий. В прошлом году я пытался использовать наименование категории (именно в таком виде, с полным путем по дереву каталога), к сожалению, это не улучшало результат классификации. На этот раз, эту колонку просто удаляем, и использовать не будем. \
Кроме того, для каждого товара есть изображение, его тоже можно использовать для классификации.
Имеющиеся данные о товаре  я собрал в одну строку при помощи простого преобразования: конкатенировал все данные в строку, удалил скобки и html теги (на мой взгляд, очень простое и эффективное решение, в прошлый раз оно тоже работало), пример: "Aksik: Зарядный кабель Borofone BX1 Lightning для айфон, 1м.   Зарядный кабель Borofone BX1 подходит для зарядки всех гаджетов и аксессуаров с разъемом  Lightning.   Поддерживает быструю зарядку.    Подходит для передачи данных.  .  Длина: 1м, Разъем: Lightning, Подерживает быструю зарядку, Максимальный ток: 2.0А, Для зарядки и синхронизации данных, Вес: 22 г. .   .  Цвет:  Черный, Белый  .  Цвет:  Белый, Черный  . . "
Данные делились на train/test в соотношении 80/20 со стратфикацией по category_id.

Одна из особенностей - большой дисбаланс: есть множество категорий, в которые попадают всего 2 товара, есть категории с тысячами товаров. \

### Бейзлайн.
В качестве бейзлайна решено взять максимально простую модель (пайплайн уже готов с 2022 года) - классификатор на основе FastText. \
Этот бейзлайн я использовал в 2022 году, и он показал лучшие результаты (1 место в общем лидерборде).
Принцип работы FastText коротко можно описать так: Выделение n-чарграмм и n-вордграмм -> Получение (обучение с учителем) ембеддингов токенов -> Усреднение полученных векторов слов по предложению (документу) -> Линейный слой с активацией Softmax. \
Важным улучшением стало увеличение веса первых слов описания text_fields товара, как несущих самую важную информацию во всём описании.
После подбора гиперпараметров удалось достичь F1=0.867 по кросс-валидации на 5 фолдах.

#### Что можно улучшить по сравнению с бейзлайном:
1. Использовать BERT. Это лучшая на снгодня модель для классификации текстовых данных, обязательно следует её попробовать.
2. Использовать изображения (мультимодальная модель). Базовой моделью здесь будет, конечно, CLIP. Эта модель, обученная для задачи Zero Shot Learning, способна давать близкие эмбеддинги для изображений и текстовых описаний.
3. Попытаться сделать стекинг 3х моделей: BERT, CLIP, FastText.
4. Посчитать эмбеддинги категорий и использовать поиск ближайших соседей (faiss/nmslib/annoy какой-нибудь) и модель для ранжирования соседей. Эмбеддинги категорий можно посчитать, например, как я делал в 2022 году - взвешенное усреднение от эмбеддинга текстового описания категории и среднего значения эмбеддингов товаров, которые в категорию попали. Более прогрессивный метод - для расчета эмбеддингов использовать metric learning (как многие делают в [этом соревновании](https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations)).
5. Попробовать балансировать классы. Обычно это приводит к тому, что модель начинает лучше предсказывать миноритарные классы, мажоритарные - хуже, а weighted F1 падает. В 2022 году это не сработало (также "мягкая" балансировка, т.е. когда дисбаланс устранялся не полностью не помогала), сейчас пробовать не стал.


### Полученные результаты BERT:
- Пробовал rubert-tiny2, LaBSE-en-ru, sbert_large_mt_nlu_ru. Делал подбор гиперпараметров с разными вариантами: с заморозкой слоев/без заморозки, с bottleneck перед финальным слоем классификатора, с дополнительными слоями и активациями. Самый простой вариант работает хорошо: mean pooling (согласно рекомендациям на huggingface) и один дополнительный слой на 840 выходов для классификации.
- Лучший результат: F1=0.8874 (LaBSE-en-ru, длина последовательности 256 токенов, Warmup scheduler, max_lr=0.00005, оптимизатор AdamW(weight_decay=1e-4), CrossEntropyLoss(label_smoothing=0.003), batch_size=32). Здесь гиперпараметры я не подбирал, модель достаточно тяжелая (и batch_size пришлось ставить меленький), скорее всего, результат можно улучшить за сутки grid search на A100, например. Этот результат получен при первой попытке обучения.
- Почти такой же результат даёт дистилированный rubert-tiny2 (F1=0.88487), многократно превосходя по производительности LaBSE-en-ru. Возможно, стоит использовать именно его. Кроме того, большое ускорение даёт обрезание последовательности до 128 токенов (F1=0.8825).
- Валидация производилась на 20% выборке (sklearn train_test_split, seed=42).

### Полученные результаты BERT + CLIP:
- CLIP (clip-vit-large-patch14) только на изображениях, без текста показал результат F1=0.64. Модель очень тяжелая, поэтому гиперпараметры почти не подбирались.
- CLIP без использования BERT (если использовать собственную модель CLIP для ембеддингов текста) показывает результат заметно хуже бейзлайна. Поэтому CLIP использовался только для получения эмбеддингов изображений.
- Эмбеддинги CLIP имеют размерность 512 - это очень много, мультимодальная модель переобучается на них, поэтому размерность я снизил до 32 (просто обрезал и сделал finetuning на тренировочной выборке с таким боттлнеком).
- При обучении мультимодальной модели я столкнулся с той же проблемой, которая описывалась на митапе Aliexpress: веса CLIP медленно уходят в 0 и не успевают обучиться (BERT успевает переобучиться, нейроны классификатора обучаются очень медленно, наоборот, BERT обучается под них), так как его качество предсказание хуже, чем у BERT на текстах. На митапе предлагали решать эту проблему через регуляризацию (но ограничивать коэффициенты не сверху, а снизу). Я использовал другой подход: обучил сначала веса слоя классификатора для CLIP (для этого эмбеддинги BERT временно заменил случайными значениями и поставил weight_decay=0.5, сохранил полученные веса, инициализировал модель заново, присвоил в state_dict() модели весам классификатора , соответствующим CLIP, сохраненные веса), затем, обучал несколько эпох с замороженным BERT, размораживал BERT и дообучал.
- В процессе обучения мультимодальной модели я не обучал CLIP, а использовал предварительно рассчитанные эмбеддинги (на которых логистическая регрессия показывает качество F1=0.64).
- Большого повышения качества добиться не получилось (порядка +0,003 к целевой метрике). Возможно, нужно лучше обучать CLIP, делать более сложную мета-модель, которая объединяет CLIP и BERT (у меня это просто один полносвязный слой), но я не пробовал, так как эта модель оказалась слишком тяжелой и обучается долго.
- Валидация производилась на 20% выборке (sklearn train_test_split, seed=42).


#### Сводная таблица результатов.
| №   | Model | Валидация | F1 |
| --- | ----------- | ----------- |:----:|
| 1. |	Fasttext | Кросс-валидация, 5 фолдов | 0.8672 |
| 2. |	LaBSE | Тестовая выборка 20%, seed=42 | 0.8874 |
| 3. |	rubert_tiny2 | Тестовая выборка 20%, seed=42 |  0.88487 |
| 4. |	CLIP (Pictures) | Тестовая выборка 20%, seed=42 | 0.64 |
| 5. |	CLIP + BERT (rubert_tiny2) | Тестовая выборка 20%, seed=42 | 0.88771 |

