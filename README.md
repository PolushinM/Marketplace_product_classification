### Описание задачи:
Научиться предсказывать категорию на основе описания, картинки и других параметрах товаров. \
Дано: около 840 категорий, 91120 товаров в трейне и 16860 в тесте.

### Краткий обзор существующих решений.
- Задача похожа на задачу, которую я решал при отборе на стажировку в 2022 году: https://github.com/PolushinM/Hierarchical-Classifier.
- Также у этой задачи много общего с задачей матчинга товаров, которая очень часто решается в маркетплейсах, много идей можно почерпнуть из этих статей: [Раз](https://habr.com/ru/company/aliexpress_russia/blog/686672/), [два](https://habr.com/ru/company/aliexpress_russia/blog/660757/).

### В полученном для работы присутствуют следующие свойства объектов:
**product_id** - идентификатор товара, который необходимо классифицировать. \
**text_fields** - словарь, сериализованный в JSON со следующими полями: \
- **title** - заголовок \
- **description** - краткое описание товара\
- **attributes** - на сайте они находятся в карточке товара (Краткое описание товара) \
- **custom_characteristics, defined_characteristics, filters** - вложенные словари с ключами – названиями полей-признаков и значениями – возможными значениями этих признаков (например, Цвет: Белый / Черный) \
- **rating** - средний рейтинг товара. Эту информацию необходимо удалить - рейтинг товара неизвестен на момент добавления его в каталог (это будет лик). \
- **sale** - флаг, обозначающий находится ли товар в распродаже. Если товар на распродаже, я добавляю в его описание слово "Распродажа". Одни категории товаров чаще других оказываются в распродаже, поэтому данное поле может иметь какое-то значение, но, скорее всего, совсем небольшое. \
**shop_id** (seller_id) - id магазина (id-продавца). Эту колонку я удалил: наименование магазина уже есть, и оно даст больше информации. \
**shop_title** - название магазина \
**category_id** - категория товара (таргет) \
**category_name** - название категории товара с точки зрения дерева категорий KazanExpress. \
Теоретически, наименование категории тоже можно использовать для классификации, но это очень сильно усложняет пайплайн. \
Например, можно вычислить эмбеддинги категории, и искать ближайших соседей для товара среди этих категорий. В 2022 году я пытался использовать наименование категории (именно в таком виде, с полным путем по дереву каталога), к сожалению, это не улучшало результат классификации. На этот раз, эта колонка просто удалена, она не использовалась. \
Кроме того, для каждого товара есть изображение, его тоже можно использовать для классификации.
Имеющиеся данные о товаре  я собрал в одну строку при помощи простого преобразования: конкатенировал все данные в строку, удалил скобки и html теги (на мой взгляд, очень простое и эффективное решение, в прошлый раз оно тоже работало). \
**Пример**: \
"Aksik: Зарядный кабель Borofone BX1 Lightning для айфон, 1м.   Зарядный кабель Borofone BX1 подходит для зарядки всех гаджетов и аксессуаров с разъемом  Lightning.   Поддерживает быструю зарядку.    Подходит для передачи данных.  .  Длина: 1м, Разъем: Lightning, Подерживает быструю зарядку, Максимальный ток: 2.0А, Для зарядки и синхронизации данных, Вес: 22 г. .   .  Цвет:  Черный, Белый  .  Цвет:  Белый, Черный  . . "
Данные делились на train/test в соотношении 80/20 со стратификацией по category_id.

Одна из особенностей - большой дисбаланс: есть множество категорий, в которые попадают всего 2 товара, есть категории с тысячами товаров. 

### Бейзлайн (fasttext_baseline.ipynb).
- В качестве бейзлайна решено взять максимально простую модель (пайплайн уже готов с 2022 года) - классификатор на основе FastText. \
- Этот бейзлайн я использовал, и он показал лучшие результаты (1 место в общем лидерборде). \
- Принцип работы FastText коротко можно описать так: Выделение n-чарграмм и n-вордграмм -> Получение (обучение с учителем) ембеддингов токенов -> Усреднение полученных векторов слов по предложению (документу) -> Линейный слой с активацией Softmax. \
- Важным улучшением стало увеличение веса первых слов описания text_fields-title товара, как несущих самую важную информацию во всём описании. \
- После подбора гиперпараметров удалось достичь F1=0.8672 по кросс-валидации на 5 фолдах.

### Что можно улучшить по сравнению с бейзлайном:
1. Использовать BERT. Это лучшая на снгодня модель для классификации текстовых данных, обязательно следует её попробовать.
2. Использовать изображения (мультимодальная модель). Базовой моделью здесь будет, конечно, CLIP. Эта модель, обученная для задачи Zero Shot Learning, способна давать близкие эмбеддинги для изображений и текстовых описаний.
3. Попытаться сделать стекинг 3х моделей: BERT, CLIP, FastText.
4. Посчитать эмбеддинги категорий и использовать поиск ближайших соседей (faiss/nmslib/annoy какой-нибудь) и модель для ранжирования соседей. Эмбеддинги категорий можно посчитать, например, как я делал в 2022 году - взвешенное усреднение от эмбеддинга текстового описания категории и среднего значения эмбеддингов товаров, которые в категорию попали. Более прогрессивный метод - для расчета эмбеддингов использовать metric learning (как многие делают в [этом соревновании](https://www.kaggle.com/competitions/learning-equality-curriculum-recommendations)).
5. Попробовать балансировать классы. Обычно это приводит к тому, что модель начинает лучше предсказывать миноритарные классы, мажоритарные - хуже, а weighted F1 падает. В 2022 году это не сработало (также "мягкая" балансировка, т.е. когда дисбаланс устранялся не полностью не помогала), сейчас пробовать не стал.

### Полученные результаты BERT (ruberttiny2.ipynb):
- Скорректировал строковое представление товара. Хотя BERT и position agnostic модель, но наиболее значимые поля лучше помещать в начало строки по двум причинам. Первое - строка обрезается по максимальному количеству токенов, и информация, расположенная в к середине или конце строки может просто пропасть. Второе - длины полей каждый раз разные, поэтому только поля, которые в строке идут первыми получают один и тот же positional encoding, что, возможно, облегчает их обработку моделью.
- Пробовал rubert-tiny2, LaBSE-en-ru, sbert_large_mt_nlu_ru. Делал подбор гиперпараметров с разными вариантами: с заморозкой слоев/без заморозки, с bottleneck перед финальным слоем классификатора, с дополнительными слоями и активациями. Самый простой вариант работает хорошо: mean pooling (согласно рекомендациям на huggingface) и один дополнительный слой на 840 выходов для классификации.
- Лучший результат: F1=0.8879 (LaBSE-en-ru, длина последовательности 256 токенов, Warmup scheduler, max_lr=0.00005, оптимизатор AdamW(weight_decay=1e-4), CrossEntropyLoss(label_smoothing=0.003), batch_size=32). Здесь гиперпараметры я не подбирал, модель достаточно тяжелая (арендовал для обучения A100 80Gb), скорее всего, результат можно улучшить за пару суток grid search. Этот результат получен при первой попытке обучения.
- Почти такой же результат даёт дистилированный rubert-tiny2 (F1=0.88487), многократно превосходя по производительности LaBSE-en-ru. Возможно, стоит использовать именно его. Кроме того, большое ускорение даёт обрезание последовательности до 128 токенов (F1=0.8825).
- Валидация производилась на 20% выборке (sklearn train_test_split, seed=42).

### Полученные результаты BERT + CLIP (clip-ruberttiny2.ipynb):
- CLIP (clip-vit-large-patch14) только на изображениях, без текста показал результат F1=0.64. Модель очень тяжелая, поэтому гиперпараметры почти не подбирались, полагаю, можно обучить её значительно лучше.
- CLIP без использования BERT (если использовать собственную модель CLIP для ембеддингов текста) показывает результат заметно хуже бейзлайна. Поэтому CLIP использовался только для получения эмбеддингов изображений.
- Эмбеддинги CLIP имеют размерность 512 - это очень много, мультимодальная модель переобучается на них, поэтому размерность я снизил до 32 (просто обрезал и сделал fine tuning на тренировочной выборке с таким боттлнеком).
- При обучении мультимодальной модели я столкнулся с той же проблемой, которая описывалась на митапе Aliexpress: веса CLIP медленно уходят в 0 и не успевают обучиться (BERT успевает переобучиться, нейроны классификатора обучаются очень медленно, наоборот, BERT обучается под них), так как его качество предсказание хуже, чем у BERT на текстах. На митапе предлагали решать эту проблему через регуляризацию (но ограничивать коэффициенты не сверху, а снизу). 
- В процессе обучения мультимодальной модели я не обучал CLIP, а использовал предварительно рассчитанные эмбеддинги (на которых логистическая регрессия показывает качество F1=0.64).
- Я пробовал следующее: обучил сначала веса слоя классификатора для CLIP (для этого эмбеддинги BERT временно заменил случайными значениями и поставил weight_decay=0.5, сохранил полученные веса, инициализировал модель заново, присвоил в state_dict() модели весам классификатора , соответствующим CLIP, сохраненные веса), затем, обучал несколько эпох с замороженным BERT, размораживал BERT и дообучал.
- Лучше всего работал подход с двумя вниманиями (простой dot attention - запрос умножается на матрицу весов, к нему применяется softmax, полученный вектор поэлементно перемножается с вектором значения): первый attention: в качестве запроса - эмбеддинг BERT, в качестве значения - эмбеддинг CLIP, второй attention, наоборот: в качестве запроса - эмбеддинг CLIP, в качестве значения - эмбеддинг BERT. Полученные вектора конкатенируются, через batchnorm поступают на линейный слой классификатора.
- Большого повышения качества добиться не получилось (порядка +0,5% к целевой метрике). Возможно, нужно лучше обучать CLIP, делать ещё более сложную мета-модель, которая объединяет CLIP и BERT, но я не пробовал, так как эта модель оказалась слишком тяжелой и обучается долго.
- Валидация производилась на 20% выборке (sklearn train_test_split, seed=42).

### Посмотрим ошибки (wrong_predictions.ipynb):
**В тестовой выборке получилось 1988 (11% от исходного количества) товаров, на которых модель (BERT + CLIP) ошиблась, смотрим, что это за товары:**

**Пример, когда модель предсказывает правильно, но категория все равно не совпадает с разметкой (таких примеров много):** \
*"{"title": "Картина по номерам на холсте с подрамником \"Двухцветный попугай\", 50x40 см"}*
*TRUE: Хобби и творчество->Создание картин, фоторамок, открыток->Картины по номерам->Другое*
*PRED: Хобби и творчество->Создание картин, фоторамок, открыток->Картины по номерам->Животные и птицы"*

**Пример, когда модель предсказывает неплохо, но немного не попадает в категорию (таких примеров большинство):** \
*"{"title": "Настенный стеллаж для хранения специй и банки для специй"}
*TRUE: Товары для дома->Товары для кухни->Порядок на кухне->Полки кухонные
*PRED: Товары для дома->Товары для кухни->Хранение продуктов->Емкости для специй и мельницы"

**Пример грубой ошибки модели (подобных ошибок среди первых 20 неверно предсказанных я нашел всего 4, т.е. 2.2% от исходного количества):** \
*"{"title": "Наклейка знак \"Не мусорить\", 18х18 см"}*
*TRUE: Товары для дома->Декор и интерьер->Таблички, номера и крючки*
*PRED: Товары для дома->Хозяйственные товары->Мусорные ведра и баки"*

### Сводная таблица результатов.
| №   | Model | Валидация | F1 |
| --- | ----------- | ----------- |:----:|
| 1. |	Fasttext | Кросс-валидация, 5 фолдов | 0.8672 |
| 2. |	LaBSE | Тестовая выборка 20%, seed=42 | 0.8879 |
| 3. |	rubert_tiny2 | Тестовая выборка 20%, seed=42 |  0.8849 |
| 4. |	CLIP (Pictures only) | Тестовая выборка 20%, seed=42 | 0.64 |
| 5. |	CLIP + BERT (rubert_tiny2) | Тестовая выборка 20%, seed=42 | 0.89022 |

### Submit
Для итогового расчета использовалась модель CLIP + BERT (clip-vit-large-patch14 + rubert_tiny2), где CLIP был предварительно обучен, сгенерированы его эмбеддинги. BERT обучался вместе с мета-моделью, содержащей 2 механизма внимания. Сначала было выполнено обучение с 20% тестовым сетом, затем, обучение с теми же гиперпараметрами на полном датасете.
