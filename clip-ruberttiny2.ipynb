{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T12:48:40.281315Z","iopub.status.busy":"2023-03-19T12:48:40.280943Z","iopub.status.idle":"2023-03-19T12:48:44.683544Z","shell.execute_reply":"2023-03-19T12:48:44.682333Z","shell.execute_reply.started":"2023-03-19T12:48:40.281282Z"},"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import multiprocessing\n","import warnings\n","import random\n","from tqdm.auto import tqdm\n","\n","from PIL import Image\n","import numpy as np \n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import ChainedScheduler, LinearLR, ExponentialLR\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import default_collate\n","\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import DataCollatorWithPadding\n","from transformers import CLIPProcessor, CLIPModel, CLIPTokenizerFast\n","\n","from utils import get_title, preprocess_text_field, MeanPooling, Attention\n","\n","def seed_everything(seed=42, deterministic=False):\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = deterministic\n","    torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","metadata":{},"source":["#### Основные настройки: seed, модель, рабочий каталог, warnings."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T12:48:44.686340Z","iopub.status.busy":"2023-03-19T12:48:44.686049Z","iopub.status.idle":"2023-03-19T12:48:48.181985Z","shell.execute_reply":"2023-03-19T12:48:48.180946Z","shell.execute_reply.started":"2023-03-19T12:48:44.686310Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["env: TOKENIZERS_PARALLELISM=false\n","Device:  cuda\n","CPU cores:  2\n"]}],"source":["SEED = 42\n","WORKDIR = '//kaggle/input/kazan-exress-1/'\n","IMAGES_FOLDER = os.path.join(WORKDIR, 'row_data/images/train/')\n","warnings.filterwarnings(\"ignore\")\n","os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'\n","seed_everything(SEED, deterministic=True)\n","\n","%env TOKENIZERS_PARALLELISM=false\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device: \", device)\n","print('CPU cores: ', multiprocessing.cpu_count())\n","\n","# =========================================================================================\n","# Configurations\n","# =========================================================================================\n","class CFG:\n","    num_workers = multiprocessing.cpu_count()\n","    clip_model = \"openai/clip-vit-large-patch14\"\n","    clip_tokenizer = CLIPTokenizerFast.from_pretrained(clip_model)\n","    clip_processor = CLIPProcessor.from_pretrained(clip_model)\n","    clip_embeddings = np.load(os.path.join(WORKDIR, 'embeddings.np.npy'))\n","    clip_cut_emb = 32\n","    bert_model = 'cointegrated/rubert-tiny2' \n","    bert_tokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny2')\n","    bert_cut_emb = None\n","    state_dict = None\n","    max_length = 256\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Преобразование входных данных"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T13:25:13.053624Z","iopub.status.busy":"2023-03-19T13:25:13.052699Z","iopub.status.idle":"2023-03-19T13:25:20.015958Z","shell.execute_reply":"2023-03-19T13:25:20.014906Z","shell.execute_reply.started":"2023-03-19T13:25:13.053574Z"},"trusted":true},"outputs":[],"source":["# Read from parquet\n","data_full = pd.read_parquet('/kaggle/input/kazanexpress-data-with-categories/train.parquet')\n","# Drop unnecessary columns\n","data_full.drop(columns=['shop_id', 'rating'], inplace=True)\n","# Convert text fields\n","data_full['title'] = data_full.text_fields.apply(get_title)\n","data_full.text_fields = data_full.text_fields.apply(preprocess_text_field)\n","# Convert \"Sale\"\n","data_full['sale'] = data_full['sale'].apply(lambda x: \"Распродажа!\" if x else \"\")  \n","data_full.fillna(value='', inplace=True)\n","# Concatenate to one string\n","data_full = data_full.assign(Document=[str(y) + ': ' + str(x) + '. ' + str(z) + '. ' + str(s) + '. ' \\\n","                                       for x, y, z, s in zip(data_full['title'], data_full['shop_title'],\n","                                                           data_full['text_fields'], data_full['sale'])])\n","\n","data_full = data_full.drop(columns=['text_fields', 'shop_title', 'sale', 'title']).reset_index(drop=True)\n","# Drop too rare values\n","drop_ids = set(data_full.category_id.value_counts()[data_full.category_id.value_counts() < 2].index)\n","data_full = data_full[~data_full['category_id'].isin(drop_ids)]\n","# Trait/test split\n","if CFG.clip_embeddings is not None:\n","    data, data_valid, clip_embeddings, clip_embeddings_valid = train_test_split(data_full, CFG.clip_embeddings, \n","                                                                    test_size=0.2, random_state=SEED, \n","                                                                    shuffle=True, stratify=data_full.category_id)\n","else:\n","    data, data_valid_stack = train_test_split(data_full, test_size=0.2, random_state=SEED, \n","                                        shuffle=True, stratify=data_full.category_id)\n","    \n","data.reset_index(drop=True, inplace=True)\n","data_valid.reset_index(drop=True, inplace=True)\n","# Fix class umbers \n","cls2id = data_full.category_id.unique()\n","id2cls = {k : v for v, k in enumerate(cls2id)}\n","\n","# del data_full\n","id2category = {k:v[15:] for k, v in zip(data_full.category_id.tolist(), data_full.category_name.tolist())}"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-03-19T13:32:39.393451Z","iopub.status.busy":"2023-03-19T13:32:39.392979Z","iopub.status.idle":"2023-03-19T13:32:40.435364Z","shell.execute_reply":"2023-03-19T13:32:40.434319Z","shell.execute_reply.started":"2023-03-19T13:32:39.393403Z"}},"source":["#### Классы датасета и модели. \n","Модель и датасет позволяют загружать как модель CLIP (собственную или с huggingface), так и готовые (ранее сгенерированные и сохранунные в numpy array) эмбеддинги CLIP."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T12:48:54.685665Z","iopub.status.busy":"2023-03-19T12:48:54.685284Z","iopub.status.idle":"2023-03-19T12:48:54.709290Z","shell.execute_reply":"2023-03-19T12:48:54.707979Z","shell.execute_reply.started":"2023-03-19T12:48:54.685625Z"},"trusted":true},"outputs":[],"source":["# =========================================================================================\n","# Dataset\n","# =========================================================================================\n","class stacked_dataset(Dataset):\n","    def __init__(self, cfg, documents:list, targets: list, \n","                 id2cls: dict, images_folder: str, \n","                 product_ids:list, clip_embeddings=None):\n","        \n","        if clip_embeddings is not None:\n","            self.use_precalculated_clip_embs = True\n","            self.clip_embeddings = clip_embeddings\n","        else:\n","            self.use_precalculated_clip_embs = False\n","        \n","        self.cfg = cfg\n","        self.data = documents\n","        self.targets = targets\n","        self.id2cls = id2cls\n","        self.images_folder = images_folder\n","        self.product_ids = product_ids\n","        \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, item):\n","        \n","        if self.use_precalculated_clip_embs:\n","            image_inputs = self.clip_embeddings[item][None, :]\n","        else:\n","            image = Image.open(os.path.join(self.images_folder, str(self.product_ids[item]) + '.jpg'))\n","            image_inputs = self.cfg.clip_processor(\n","                    text=None,\n","                    images=image,\n","                    return_tensors='pt'\n","                )['pixel_values']\n","        \n","        text_inputs=self.cfg.bert_tokenizer(\n","                self.data[item], \n","                return_tensors=None, \n","                add_special_tokens=True, \n","                max_length=self.cfg.max_length,\n","                truncation=True\n","            )\n","                \n","        return text_inputs, image_inputs, self.id2cls[self.targets[item]]\n","\n","# =========================================================================================\n","# Unsupervised model\n","# =========================================================================================\n","\n","class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        mean_embeddings = sum_embeddings / sum_mask\n","        return mean_embeddings\n","\n","class Attention(nn.Module):\n","    def __init__(self, query_dim, value_dim):\n","        super(Attention, self).__init__()\n","        self.fc = nn.Linear(query_dim, value_dim, bias=False)\n","    def forward(self, query_emb, value_emb):\n","        attention = torch.sigmoid(self.fc(query_emb))\n","        return value_emb * attention\n","    \n","class STACKED_CLF(nn.Module):\n","    def __init__(self, cfg, n_classes):\n","        super().__init__()\n","        # Configurations, CLIP and BERT models loading\n","        self.cfg = cfg\n","        if cfg.clip_embeddings is None:\n","            self.clip_config = AutoConfig.from_pretrained(cfg.model)\n","            self.clip_model = CLIPModel.from_pretrained(cfg.model)\n","        else: \n","            self.use_precalculated_clip_embs = True\n","            \n","        self.bert_config = AutoConfig.from_pretrained(cfg.bert_model)\n","        self.bert_model = AutoModel.from_pretrained(cfg.bert_model, config = self.bert_config)\n","        self.bert_pool = MeanPooling()\n","        # CLIP embeddings from model or precalculated\n","        if cfg.bert_cut_emb is None:\n","            self.hidden_dim = self.bert_model.config.hidden_size + cfg.clip_cut_emb\n","        else:\n","            self.hidden_dim = cfg.bert_cut_emb + cfg.clip_cut_emb\n","        # Attentions\n","        self.attention_clip = Attention(self.hidden_dim-cfg.clip_cut_emb, cfg.clip_cut_emb)\n","        self.attention_bert = Attention(cfg.clip_cut_emb, self.hidden_dim-cfg.clip_cut_emb)\n","        # Classifier\n","        self.bn = nn.BatchNorm1d(self.hidden_dim)\n","        self.clf = nn.Linear(self.hidden_dim, n_classes)\n","\n","    def forward(self, text_inputs, image_inputs):\n","        # Get BERT embeddings from text\n","        text_emb = self.bert_model(**text_inputs)\n","        text_emb = self.bert_pool(text_emb.last_hidden_state, text_inputs['attention_mask']) \n","        # Get CLIP embeddings from pictures\n","        if self.use_precalculated_clip_embs:\n","            img_emb = image_inputs \n","        else:\n","            img_emb = self.clip_model.get_image_features(image_inputs)\n","        # Cut embeddings\n","        text_emb = text_emb[:, :self.cfg.bert_cut_emb]\n","        img_emb = img_emb[:, :self.cfg.clip_cut_emb]\n","        # Apply attentions\n","        img_emb = self.attention_clip(text_emb, img_emb)\n","        text_emb = self.attention_bert(img_emb, text_emb)\n","        # Concatenate BERT and CLIP embeddings\n","        emb  = torch.cat([text_emb, img_emb], dim=1).float()\n","        # Classifier\n","        cls = self.clf(self.bn(emb))\n","        return cls"]},{"cell_type":"markdown","metadata":{},"source":["#### Функция collate_fn и даталоадер:"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T12:48:54.711665Z","iopub.status.busy":"2023-03-19T12:48:54.710925Z","iopub.status.idle":"2023-03-19T12:48:54.731828Z","shell.execute_reply":"2023-03-19T12:48:54.730616Z","shell.execute_reply.started":"2023-03-19T12:48:54.711627Z"},"trusted":true},"outputs":[],"source":["transformers_collator = DataCollatorWithPadding(tokenizer = CFG.bert_tokenizer, padding = 'longest')\n","\n","def custom_collate(batch):\n","    texts_batch = []\n","    images_batch = []\n","    targets_batch = []\n","    for item in batch:\n","        texts_batch.append(item[0])\n","        images_batch.append(item[1][0])\n","        targets_batch.append(item[2])\n","    text_inputs = transformers_collator(texts_batch)\n","    return text_inputs, default_collate(images_batch), default_collate(targets_batch)\n","\n","\n","train_loader = DataLoader(\n","    stacked_dataset(CFG, documents=data.Document.tolist(), targets=data.category_id.tolist(), \n","                  id2cls=id2cls, images_folder=IMAGES_FOLDER, \n","                  product_ids=data.product_id.tolist(), clip_embeddings=clip_embeddings), \n","    batch_size = 256, \n","    shuffle = True, \n","    collate_fn = custom_collate,\n","    num_workers = CFG.num_workers, \n","    pin_memory = True, \n","    drop_last = False\n",")\n","\n","valid_loader = DataLoader(\n","    stacked_dataset(CFG, documents=data_valid.Document.tolist(), targets=data_valid.category_id.tolist(), \n","                  id2cls=id2cls, images_folder=IMAGES_FOLDER, \n","                  product_ids=data_valid.product_id.tolist(), clip_embeddings=clip_embeddings_valid), \n","    batch_size = 256, \n","    shuffle = False, \n","    collate_fn = custom_collate,\n","    num_workers = CFG.num_workers, \n","    pin_memory = True, \n","    drop_last = False\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Train loop."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T17:28:45.450297Z","iopub.status.busy":"2023-03-19T17:28:45.449820Z","iopub.status.idle":"2023-03-19T17:28:45.473734Z","shell.execute_reply":"2023-03-19T17:28:45.472655Z","shell.execute_reply.started":"2023-03-19T17:28:45.450256Z"},"trusted":true},"outputs":[],"source":["def train(model, train_loader, test_loader, \n","          epochs=20, lr=0.0001, checkpoint_period=None, weight_decay=1e-7,\n","          warmup_epochs=2, gamma=0.93, verbose=True):\n","    \n","    opt = AdamW(model.parameters(), lr=lr * gamma ** -warmup_epochs, weight_decay=weight_decay)\n","    \n","    model.to(device)\n","    loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.01)   \n","    opt.zero_grad() \n","    torch.cuda.empty_cache()\n","    # gc.collect()\n","    scheduler = ChainedScheduler([LinearLR(opt, start_factor=0.02, total_iters=warmup_epochs),\n","                                  ExponentialLR(opt, gamma=gamma)])\n","    if checkpoint_period is None:\n","        checkpoint_period = len(train_loader)\n","    \n","    max_f1 = 0.\n","    best_epoch = 0\n","    \n","    for epoch in tqdm(range(1, epochs+1, 1)):\n","        # TRAIN\n","        model.train()\n","        loss_avg = 0.\n","        if verbose:\n","            print(f'Epoch={epoch}')\n","            print(f'Lr: {scheduler.get_last_lr()[0]:.9f}')\n","        for step, batch in tqdm(enumerate(train_loader), total=len(train_loader), disable = not verbose):\n","            text_input = batch[0].to(device)\n","            image_input = batch[1].to(device)\n","            target = batch[2].to(device)\n","            output = model(text_input, image_input)\n","            loss = loss_fn(output, target)\n","            loss.backward()\n","            opt.step()\n","            opt.zero_grad()\n","            loss_avg += loss.item() / checkpoint_period\n","            if step % checkpoint_period == checkpoint_period - 1:\n","                if verbose:\n","                    print(f'Step={step+1}, Train loss={loss_avg:.6f}')\n","                loss_avg = 0.\n","                torch.save(model.state_dict(), os.path.join(WORKDIR, 'checkpoints/checkpoint.pt'))\n","                model.eval()\n","                grun_truth = []\n","                predicted = []                \n","                with torch.no_grad():\n","                    precision = 0.\n","                    recall = 0.\n","                    total = min(len(test_loader), (checkpoint_period // 2))\n","                    for step, batch in enumerate(test_loader):\n","                        text_input = batch[0].to(device)\n","                        image_input = batch[1].to(device)\n","                        target = batch[2].to(device)\n","                        output = model(text_input, image_input)\n","                        loss = loss_fn(output, target)\n","                        loss_avg += loss.item() / total\n","                        grun_truth.append(target.cpu())\n","                        predicted.append(output.argmax(dim=1).cpu())\n","                        if step >= checkpoint_period // 2 - 1:\n","                            break\n","                weighted_f1 = f1_score(np.concatenate(grun_truth), np.concatenate(predicted), average='weighted')\n","                \n","                if weighted_f1 > max_f1:\n","                    max_f1 = weighted_f1\n","                    best_epoch = epoch\n","                    torch.save(model.state_dict(), os.path.join(WORKDIR, 'best.pt'))\n","\n","                if verbose:\n","                    print(f\"F1={weighted_f1:.5f}\")\n","                    print(f'Eval loss={loss_avg:.5f}\\n')\n","                loss_avg = 0.\n","                model.train()\n","                scheduler.step()\n","    return max_f1, best_epoch"]},{"cell_type":"markdown","metadata":{},"source":["#### Создание модели."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T12:48:54.757877Z","iopub.status.busy":"2023-03-19T12:48:54.756889Z","iopub.status.idle":"2023-03-19T12:48:58.235133Z","shell.execute_reply":"2023-03-19T12:48:58.234079Z","shell.execute_reply.started":"2023-03-19T12:48:54.757842Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["model = STACKED_CLF(CFG, len(cls2id)).to(device)\n","if CFG.state_dict is not None:\n","    model.load_state_dict(CFG.state_dict)\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["#### Обучение модели."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-03-19T12:48:58.237026Z","iopub.status.busy":"2023-03-19T12:48:58.236632Z","iopub.status.idle":"2023-03-19T13:20:36.085588Z","shell.execute_reply":"2023-03-19T13:20:36.084374Z","shell.execute_reply.started":"2023-03-19T12:48:58.236987Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe539020ea8f4bfd99f91da9e2c6f7dd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch=1\n","Lr: 0.000009250\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98b588198961462e9f2cbfec2a5b55a6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=6.208466\n","F1=0.17484\n","Eval loss=5.42725\n","\n","Epoch=2\n","Lr: 0.000219355\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37fd1894afd34476a13dfce2a47fa232","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=2.135760\n","F1=0.79485\n","Eval loss=0.95374\n","\n","Epoch=3\n","Lr: 0.000400000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05d9c54ea5c54f128a8f189c9feb2215","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.659543\n","F1=0.86740\n","Eval loss=0.61421\n","\n","Epoch=4\n","Lr: 0.000372000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c814a14137a497f96b8663b2a0952a6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.335750\n","F1=0.88267\n","Eval loss=0.57403\n","\n","Epoch=5\n","Lr: 0.000345960\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc28a8b7de514632ae929f44b2ecfd85","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.225562\n","F1=0.88743\n","Eval loss=0.57602\n","\n","Epoch=6\n","Lr: 0.000321743\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38177e5c051741efb1324b318938e32d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.178988\n","F1=0.88656\n","Eval loss=0.58686\n","\n","Epoch=7\n","Lr: 0.000299221\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ecf7a6e7e08c49b3abd7acfa49bdd4cb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.159071\n","F1=0.88829\n","Eval loss=0.60563\n","\n","Epoch=8\n","Lr: 0.000278275\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd72ff69750343b8ac1deb05e63bed75","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.150054\n","F1=0.88808\n","Eval loss=0.62492\n","\n","Epoch=9\n","Lr: 0.000258796\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f3af74237664b70913f4de9a5d133e7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.145846\n","F1=0.89022\n","Eval loss=0.62536\n","\n","Epoch=10\n","Lr: 0.000240680\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f9cd82f5dab42c68e2f48950b0c6f00","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.142568\n","F1=0.88927\n","Eval loss=0.63574\n","\n","Epoch=11\n","Lr: 0.000223833\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3c443487bc74bb8a335a2193d3a9f91","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.140059\n","F1=0.88901\n","Eval loss=0.64354\n","\n","Epoch=12\n","Lr: 0.000208164\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d19e7a3c2c8747948ed354c555ee9525","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.138888\n","F1=0.88828\n","Eval loss=0.65133\n","\n","Epoch=13\n","Lr: 0.000193593\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65eb2ed11c1d42238ff05ebe217221be","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.137706\n","F1=0.88890\n","Eval loss=0.65808\n","\n","Epoch=14\n","Lr: 0.000180041\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"169c7b056a974318974c18ef10eda500","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.136736\n","F1=0.88795\n","Eval loss=0.66512\n","\n","Epoch=15\n","Lr: 0.000167439\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6890f162d2b946e9b17020834cc13b4d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/285 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step=285, Train loss=0.135940\n","F1=0.88680\n","Eval loss=0.67078\n","\n"]},{"data":{"text/plain":["(0.8902174169948255, 9)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["train(model, train_loader, valid_loader, warmup_epochs=2, epochs=15, lr=0.0004, gamma=0.93)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"vscode":{"interpreter":{"hash":"73baa7047eb6901c2be83950c21fe663ea57cccf327cab8d2ef6784beacf294e"}}},"nbformat":4,"nbformat_minor":4}
